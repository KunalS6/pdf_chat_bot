# ğŸ“„ PDF Chatbot (RAG) with Groq, LangChain & Streamlit

An interactive **â€œchat with your PDFâ€** app built with **Streamlit**, **LangChain 1.x**, **ChromaDB**, and **Groq LLaMA 3.1**.  
Upload any PDF and ask questions; the bot answers **only from the document content** using a Retrieval-Augmented Generation (RAG) pipeline. [web:241][web:243][web:166]

---

## âœ¨ Features

- ğŸ“¤ Upload a PDF and chat with it in your browser.
- ğŸ§  Conversational memory across turns (context-aware questions).
- ğŸ“š RAG pipeline: chunking, embeddings, and retrieval via **ChromaDB**.
- âš¡ Fast LLM responses using **Groq** (`llama-3.1-8b-instant`).
- ğŸ’¾ Persistent vector store folder (`chroma_store`).
- â¬‡ï¸ Export full chat history as CSV.
- ğŸ—‘ Clear PDF, vectors, and chat in one click.

---

## ğŸ§± Tech Stack

- **Frontend:** Streamlit
- **LLM:** Groq `llama-3.1-8b-instant` via `langchain-groq` [web:166][web:165]
- **RAG Orchestration:** LangChain 1.x (LCEL `Runnable` + memory) [web:173]
- **Vector Store:** ChromaDB
- **Embeddings:** Local transformer model (`sentence-transformers/all-MiniLM-L6-v2`) via `transformers` + `torch`
- **Document Loader:** `PyPDFLoader`
- **Chunking:** `RecursiveCharacterTextSplitter`

---

## ğŸ“‚ Project Structure

```text
.
â”œâ”€â”€ app.py             # Main Streamlit app (UI + RAG logic)
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ chroma_store/      # Persistent ChromaDB directory (auto-created)
